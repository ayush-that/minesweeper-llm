{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper LLM Competition - SFT + GRPO Training Pipeline\n",
    "\n",
    "## Model: Qwen2.5-14B-Instruct\n",
    "## Strategy: 3-Tier Solver -> 50K SFT Dataset -> GRPO Refinement\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load pre-generated training data (50K examples from forward-gameplay solver)\n",
    "2. SFT warmup: 1 epoch on solver-labeled optimal moves  \n",
    "3. GRPO refinement: 1200 steps with 3 reward functions (format, gameplay, strategic)\n",
    "4. Save merged model for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Environment Setup & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VLLM_USE_TRITON_FLASH_ATTN\"] = \"0\"  # ROCm fix for Qwen2.5 SWA\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"  # Don't try to download, use local cache\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = (\n",
    "    8192  # Handle large frontier format prompts (50x50 boards can reach ~6K tokens)\n",
    ")\n",
    "lora_rank = 64  # High rank for complex reasoning task\n",
    "\n",
    "# Use local cache path directly (HF cache is read-only)\n",
    "model_name = \"/root/.cache/huggingface/models--Qwen--Qwen2.5-14B-Instruct/snapshots/cf98f3b3bbb457ad9e2bb7baf9a0125b6b88caa8\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    load_in_4bit=False,\n",
    "    max_seq_length=max_seq_length,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=lora_rank * 2,  # alpha = 2 * rank\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "# Print trainable parameter count\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    f\"Trainable: {trainable / 1e6:.1f}M / {total / 1e9:.1f}B ({trainable / total * 100:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Load Training Data & Game Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "# ================================================================\n",
    "# Game Engine (needed for reward functions)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "class MinesweeperGame:\n",
    "    \"\"\"Minesweeper game reconstructed from stored mine positions.\"\"\"\n",
    "\n",
    "    def __init__(self, rows, cols, mine_positions):\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.mine_set = set(tuple(p) for p in mine_positions)\n",
    "        self.num_mines = len(self.mine_set)\n",
    "\n",
    "        # Calculate numbers\n",
    "        self._board = [[0] * cols for _ in range(rows)]\n",
    "        for r, c in self.mine_set:\n",
    "            self._board[r][c] = -1\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if self._board[r][c] == -1:\n",
    "                    continue\n",
    "                count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = r + dr, c + dc\n",
    "                        if (\n",
    "                            0 <= nr < rows\n",
    "                            and 0 <= nc < cols\n",
    "                            and self._board[nr][nc] == -1\n",
    "                        ):\n",
    "                            count += 1\n",
    "                self._board[r][c] = count\n",
    "\n",
    "        self.revealed = set()\n",
    "        self.flagged = set()\n",
    "        self._state = \"ongoing\"\n",
    "\n",
    "    def reveal(self, r, c):\n",
    "        \"\"\"Reveal with flood fill. Returns 'mine', 'ok', or 'win'.\"\"\"\n",
    "        if (r, c) in self.mine_set:\n",
    "            self._state = \"failed\"\n",
    "            return \"mine\"\n",
    "        stack = [(r, c)]\n",
    "        while stack:\n",
    "            cr, cc = stack.pop()\n",
    "            if (cr, cc) in self.revealed:\n",
    "                continue\n",
    "            self.revealed.add((cr, cc))\n",
    "            if self._board[cr][cc] == 0:\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = cr + dr, cc + dc\n",
    "                        if (\n",
    "                            0 <= nr < self.rows\n",
    "                            and 0 <= nc < self.cols\n",
    "                            and (nr, nc) not in self.revealed\n",
    "                            and (nr, nc) not in self.flagged\n",
    "                        ):\n",
    "                            stack.append((nr, nc))\n",
    "        # Check win\n",
    "        safe_total = self.rows * self.cols - self.num_mines\n",
    "        if len(self.revealed) >= safe_total:\n",
    "            self._state = \"success\"\n",
    "            return \"win\"\n",
    "        return \"ok\"\n",
    "\n",
    "    def flag(self, r, c):\n",
    "        self.flagged.add((r, c))\n",
    "\n",
    "    def get_board(self):\n",
    "        board = [[\".\" for _ in range(self.cols)] for _ in range(self.rows)]\n",
    "        for r, c in self.revealed:\n",
    "            board[r][c] = str(self._board[r][c])\n",
    "        for r, c in self.flagged:\n",
    "            board[r][c] = \"F\"\n",
    "        return board\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "\n",
    "\n",
    "def parse_llm_action(response):\n",
    "    \"\"\"Extract JSON action from LLM response. Returns last valid match.\"\"\"\n",
    "    best = None\n",
    "    for match in re.finditer(r\"\\{[^{}]*\\}\", response):\n",
    "        try:\n",
    "            action = json.loads(match.group())\n",
    "            if (\n",
    "                \"type\" in action\n",
    "                and \"row\" in action\n",
    "                and \"col\" in action\n",
    "                and action[\"type\"] in [\"reveal\", \"flag\"]\n",
    "            ):\n",
    "                action[\"row\"] = int(action[\"row\"])\n",
    "                action[\"col\"] = int(action[\"col\"])\n",
    "                best = action\n",
    "        except (json.JSONDecodeError, ValueError, TypeError):\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "\n",
    "def reconstruct_game(mine_positions, rows, cols, revealed_positions, flagged_positions):\n",
    "    \"\"\"Reconstruct game state from stored data.\"\"\"\n",
    "    game = MinesweeperGame(rows, cols, mine_positions)\n",
    "    # Re-reveal all cells (with flood fill)\n",
    "    for r, c in revealed_positions:\n",
    "        if (r, c) not in game.revealed:\n",
    "            game.reveal(r, c)\n",
    "    # Re-flag\n",
    "    for r, c in flagged_positions:\n",
    "        game.flag(r, c)\n",
    "    # Reset state to ongoing (we're reconstructing mid-game)\n",
    "    game._state = \"ongoing\"\n",
    "    return game\n",
    "\n",
    "\n",
    "print(\"Game engine loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify external dependencies: solver.py and generate_data.py\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "for module_name, filepath in [\n",
    "    (\"solver\", \"/workspace/solver.py\"),\n",
    "    (\"generate_data\", \"/workspace/generate_data.py\"),\n",
    "]:\n",
    "    assert os.path.exists(filepath), f\"MISSING: {filepath}\"\n",
    "    spec = importlib.util.spec_from_file_location(module_name, filepath)\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    print(f\"  {module_name}: OK ({os.path.getsize(filepath) / 1024:.1f} KB)\")\n",
    "\n",
    "# Quick solver sanity check\n",
    "from solver import solve_board\n",
    "\n",
    "board = [[\"1\", \"1\", \"1\"], [\"1\", \".\", \"1\"], [\"1\", \"1\", \"1\"]]\n",
    "solver = solve_board(board, 3, 3, 1, full=True)\n",
    "moves = solver.get_certain_moves()\n",
    "print(\n",
    "    f\"  Solver test: {len(moves)} certain moves on 3x3 with 1 mine -> {'PASS' if len(moves) == 1 else 'FAIL'}\"\n",
    ")\n",
    "print(\"\\nAll external files verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load Pre-Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-generated data from generate_data.py\n",
    "# FILTER: Skip 50x50 boards â€” competition max is < 50x50\n",
    "data_file = \"minesweeper_training_data.jsonl\"\n",
    "\n",
    "raw_data = []\n",
    "skipped_50x50 = 0\n",
    "with open(data_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line.strip())\n",
    "        if ex.get(\"board_size\") == \"50x50\":\n",
    "            skipped_50x50 += 1\n",
    "            continue\n",
    "        raw_data.append(ex)\n",
    "\n",
    "print(f\"Loaded {len(raw_data)} examples (filtered {skipped_50x50} x 50x50 boards)\")\n",
    "\n",
    "# Show distribution\n",
    "stage_counts = defaultdict(int)\n",
    "size_counts = defaultdict(int)\n",
    "deducible_count = 0\n",
    "for e in raw_data:\n",
    "    stage_counts[e[\"game_stage\"]] += 1\n",
    "    size_counts[e[\"board_size\"]] += 1\n",
    "    if e[\"is_deducible\"]:\n",
    "        deducible_count += 1\n",
    "\n",
    "print(\"\\nBoard size distribution:\")\n",
    "for size in sorted(size_counts.keys(), key=lambda x: int(x.split(\"x\")[0])):\n",
    "    cnt = size_counts[size]\n",
    "    print(f\"  {size}: {cnt} ({cnt / len(raw_data) * 100:.1f}%)\")\n",
    "\n",
    "print(\"\\nGame stage distribution:\")\n",
    "for stage in [\"opening\", \"early\", \"mid\", \"late\", \"endgame\", \"near_failure\"]:\n",
    "    cnt = stage_counts.get(stage, 0)\n",
    "    print(f\"  {stage}: {cnt} ({cnt / len(raw_data) * 100:.1f}%)\")\n",
    "\n",
    "print(\n",
    "    f\"\\nDeducible: {deducible_count}/{len(raw_data)} ({deducible_count / len(raw_data) * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Show example\n",
    "ex = raw_data[0]\n",
    "msgs = json.loads(ex[\"messages\"])\n",
    "print(\"\\nExample prompt (first 300 chars):\")\n",
    "print(msgs[1][\"content\"][:300])\n",
    "print(f\"\\nExample response: {msgs[2]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Prepare SFT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SFT dataset: parse messages from JSON strings to lists\n",
    "sft_items = []\n",
    "for ex in raw_data:\n",
    "    messages = json.loads(ex[\"messages\"])  # Parse JSON string -> list of dicts\n",
    "    sft_items.append({\"messages\": messages})\n",
    "\n",
    "sft_dataset = Dataset.from_list(sft_items)\n",
    "print(f\"SFT dataset: {len(sft_dataset)} examples\")\n",
    "\n",
    "# Verify format\n",
    "assert isinstance(sft_dataset[0][\"messages\"], list), \"Messages must be a list!\"\n",
    "assert isinstance(sft_dataset[0][\"messages\"][0], dict), \"Each message must be a dict!\"\n",
    "assert \"role\" in sft_dataset[0][\"messages\"][0], \"Messages must have 'role' key!\"\n",
    "print(\"Format check passed: messages is list of dicts with role/content\")\n",
    "\n",
    "# Verify tokenization works\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    sft_dataset[0][\"messages\"],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=False,\n",
    ")\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Example token length: {test_tokens.input_ids.shape[1]}\")\n",
    "print(\"First 200 chars of formatted text:\")\n",
    "print(test_text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: SFT Training (Phase 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "# Formatting function required by Unsloth's SFTTrainer\n",
    "# Must always return a list of strings\n",
    "def formatting_func(examples):\n",
    "    \"\"\"Apply chat template to convert messages to training text.\"\"\"\n",
    "    messages = examples[\"messages\"]\n",
    "    # Single example: messages is a list of dicts [{role:..., content:...}, ...]\n",
    "    # Batch: messages is a list of lists [[{role:..., content:...}, ...], ...]\n",
    "    if (\n",
    "        isinstance(messages, list)\n",
    "        and len(messages) > 0\n",
    "        and isinstance(messages[0], dict)\n",
    "    ):\n",
    "        # Single example - wrap in list\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        return [text]\n",
    "    else:\n",
    "        # Batch\n",
    "        return [\n",
    "            tokenizer.apply_chat_template(\n",
    "                msgs, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            for msgs in messages\n",
    "        ]\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"sft_checkpoint\",\n",
    "    per_device_train_batch_size=2,  # Reduced for 8192 seq length\n",
    "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,  # 1 epoch to avoid memorization\n",
    "    optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    max_seq_length=max_seq_length,\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=sft_dataset,\n",
    "    args=sft_config,\n",
    "    formatting_func=formatting_func,\n",
    ")\n",
    "\n",
    "print(\"SFT config:\")\n",
    "print(f\"  Epochs: {sft_config.num_train_epochs}\")\n",
    "print(\n",
    "    f\"  Batch: {sft_config.per_device_train_batch_size} x {sft_config.gradient_accumulation_steps} = {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\"\n",
    ")\n",
    "print(f\"  LR: {sft_config.learning_rate}\")\n",
    "print(f\"  Max seq length: {sft_config.max_seq_length}\")\n",
    "print(\n",
    "    f\"  Steps: ~{len(sft_dataset) // (sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps)}\"\n",
    ")\n",
    "\n",
    "print(\"\\nStarting SFT training...\")\n",
    "sft_trainer.train()\n",
    "print(\"SFT training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Save SFT Checkpoint & Quick Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save SFT checkpoint\n",
    "model.save_pretrained(\"sft_checkpoint\")\n",
    "tokenizer.save_pretrained(\"sft_checkpoint\")\n",
    "print(\"SFT checkpoint saved!\")\n",
    "\n",
    "# Quick evaluation after SFT\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "FRONTIER_THRESHOLD = 16  # Must match generate_data.py and agents/minesweeper_agent.py\n",
    "\n",
    "\n",
    "def build_eval_prompt(board, rows, cols, mines, flags):\n",
    "    \"\"\"Build eval prompt matching training data format exactly.\"\"\"\n",
    "    mines_left = mines - flags\n",
    "    if rows <= FRONTIER_THRESHOLD and cols <= FRONTIER_THRESHOLD:\n",
    "        grid = \"\\n\".join(\"\".join(r) for r in board)\n",
    "        return f'MINESWEEPER {rows}x{cols} MINES:{mines} FLAGS:{flags} LEFT:{mines_left}\\n{grid}\\nRULES: .=hidden F=flag 0-8=adjacent mines\\n- If number N has N flags around it, remaining hidden neighbors are SAFE->reveal\\n- If number N needs (N-flags) more mines and has exactly that many hidden neighbors, all are MINES->flag\\n- Flag certain mines FIRST, then reveal certain safe cells\\n- NEVER act on already revealed or flagged cells\\nOutput ONLY: {{\"type\":\"reveal\"|\"flag\",\"row\":R,\"col\":C}}'\n",
    "    else:\n",
    "        # Frontier format - matches generate_data.py and agent exactly\n",
    "        frontier_info = []\n",
    "        all_hidden_near_numbers = set()\n",
    "        for r in range(rows):\n",
    "            for c in range(cols):\n",
    "                if board[r][c] not in \"012345678\":\n",
    "                    continue\n",
    "                num = int(board[r][c])\n",
    "                fl = sum(\n",
    "                    1\n",
    "                    for dr in [-1, 0, 1]\n",
    "                    for dc in [-1, 0, 1]\n",
    "                    if not (dr == 0 and dc == 0)\n",
    "                    and 0 <= r + dr < rows\n",
    "                    and 0 <= c + dc < cols\n",
    "                    and board[r + dr][c + dc] == \"F\"\n",
    "                )\n",
    "                hidden = [\n",
    "                    (r + dr, c + dc)\n",
    "                    for dr in [-1, 0, 1]\n",
    "                    for dc in [-1, 0, 1]\n",
    "                    if not (dr == 0 and dc == 0)\n",
    "                    and 0 <= r + dr < rows\n",
    "                    and 0 <= c + dc < cols\n",
    "                    and board[r + dr][c + dc] == \".\"\n",
    "                ]\n",
    "                if hidden:\n",
    "                    for h in hidden:\n",
    "                        all_hidden_near_numbers.add(h)\n",
    "                    hs = \"\".join(f\"({hr},{hc})\" for hr, hc in hidden)\n",
    "                    frontier_info.append(f\"R{r}C{c}={num} flags:{fl} hidden:[{hs}]\")\n",
    "\n",
    "        total_hidden = sum(\n",
    "            1 for r in range(rows) for c in range(cols) if board[r][c] == \".\"\n",
    "        )\n",
    "        interior_count = total_hidden - len(all_hidden_near_numbers)\n",
    "        frontier_str = \"\\n\".join(frontier_info[:200])  # Match training data: 200 cap\n",
    "        hidden_near_str = \"\".join(\n",
    "            f\"({r},{c})\" for r, c in sorted(all_hidden_near_numbers)[:100]\n",
    "        )\n",
    "\n",
    "        return f'MINESWEEPER {rows}x{cols} MINES:{mines} FLAGS:{flags} LEFT:{mines_left}\\nFRONTIER (numbered cells with hidden neighbors):\\n{frontier_str}\\nHIDDEN NEAR NUMBERS: {hidden_near_str}\\nTOTAL HIDDEN: {total_hidden} INTERIOR(no adj number): {interior_count}\\nRULES: .=hidden F=flag 0-8=adjacent mines\\n- If number N has N flags around it, remaining hidden neighbors are SAFE->reveal\\n- If number N needs (N-flags) more mines and has exactly that many hidden neighbors, all are MINES->flag\\n- Flag certain mines FIRST, then reveal certain safe cells\\n- NEVER act on already revealed or flagged cells\\nOutput ONLY: {{\"type\":\"reveal\"|\"flag\",\"row\":R,\"col\":C}}'\n",
    "\n",
    "\n",
    "def quick_eval(model, tokenizer, num_games=20, board_configs=None):\n",
    "    \"\"\"Quick evaluation across board sizes. Continues after invalid moves (like competition).\"\"\"\n",
    "    if board_configs is None:\n",
    "        board_configs = [\n",
    "            (6, 6, 5, 5),\n",
    "            (10, 10, 15, 5),\n",
    "            (16, 16, 40, 5),\n",
    "            (20, 20, 60, 5),\n",
    "        ]\n",
    "\n",
    "    results = {}\n",
    "    for rows, cols, mines, n_games in board_configs:\n",
    "        valid_json = 0\n",
    "        valid_moves = 0\n",
    "        invalid_moves = 0\n",
    "        total_moves = 0\n",
    "        wins = 0\n",
    "\n",
    "        for seed in range(n_games):\n",
    "            rng = random.Random(seed + 10000)\n",
    "            positions = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "            mine_pos = rng.sample(positions, mines)\n",
    "            game = MinesweeperGame(rows, cols, mine_pos)\n",
    "\n",
    "            # Random first reveal\n",
    "            safe = [\n",
    "                (r, c)\n",
    "                for r in range(rows)\n",
    "                for c in range(cols)\n",
    "                if (r, c) not in game.mine_set\n",
    "            ]\n",
    "            first = rng.choice(safe)\n",
    "            game.reveal(*first)\n",
    "\n",
    "            for move_i in range(min(10, rows * cols)):\n",
    "                if game.state != \"ongoing\":\n",
    "                    if game.state == \"success\":\n",
    "                        wins += 1\n",
    "                    break\n",
    "\n",
    "                board = game.get_board()\n",
    "                flags = len(game.flagged)\n",
    "                prompt = build_eval_prompt(board, rows, cols, mines, flags)\n",
    "\n",
    "                sys_prompt = \"You are an expert Minesweeper AI. Analyze constraints and output ONLY a valid JSON action. No explanation.\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=64,\n",
    "                        temperature=1.0,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                    )\n",
    "                response = tokenizer.decode(\n",
    "                    output[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "                )\n",
    "                action = parse_llm_action(response)\n",
    "\n",
    "                total_moves += 1\n",
    "                if action is not None:\n",
    "                    valid_json += 1\n",
    "                    r_act, c_act = action[\"row\"], action[\"col\"]\n",
    "                    if 0 <= r_act < rows and 0 <= c_act < cols:\n",
    "                        cell_val = board[r_act][c_act]\n",
    "                        if cell_val == \".\":\n",
    "                            valid_moves += 1\n",
    "                            if action[\"type\"] == \"reveal\":\n",
    "                                game.reveal(r_act, c_act)\n",
    "                            elif action[\"type\"] == \"flag\":\n",
    "                                game.flag(r_act, c_act)\n",
    "                        else:\n",
    "                            # Invalid target (already revealed/flagged) - count penalty but continue\n",
    "                            invalid_moves += 1\n",
    "                    else:\n",
    "                        # Out of bounds - count penalty but continue\n",
    "                        invalid_moves += 1\n",
    "                else:\n",
    "                    # Invalid JSON - count penalty but continue\n",
    "                    invalid_moves += 1\n",
    "\n",
    "        json_rate = valid_json / max(total_moves, 1) * 100\n",
    "        move_rate = valid_moves / max(total_moves, 1) * 100\n",
    "        results[f\"{rows}x{cols}\"] = (json_rate, move_rate, total_moves, wins)\n",
    "        print(\n",
    "            f\"  {rows}x{cols}: JSON={json_rate:.0f}% ValidMove={move_rate:.0f}% Wins={wins}/{n_games} Invalid={invalid_moves} ({total_moves} moves)\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Post-SFT evaluation:\")\n",
    "sft_results = quick_eval(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: GRPO Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Reward Function 1: Format Reward (weight: 1.0)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward valid JSON action format.\n",
    "    Valid JSON with correct keys -> +1.0\n",
    "    Invalid -> -3.0\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        response = (\n",
    "            completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        )\n",
    "        action = parse_llm_action(response)\n",
    "        if action is not None:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(-3.0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Reward Function 2: Gameplay Reward (weight: 2.0)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def gameplay_reward(completions, **kwargs):\n",
    "    \"\"\"Score gameplay quality by reconstructing game and simulating the action.\n",
    "\n",
    "    Uses reconstruct_game() + actual reveal/flag for accurate win detection\n",
    "    including 0-cell flood-fill cascades.\n",
    "\n",
    "    Win requires BOTH: all mines flagged AND all safe cells revealed.\n",
    "    This matches the competition specification exactly.\n",
    "\n",
    "    Scoring (raw, normalized by /25):\n",
    "    - Out of bounds:        -15 -> -0.60\n",
    "    - Already revealed:     -12 -> -0.48\n",
    "    - Already flagged:       -8 -> -0.32\n",
    "    - Flag non-mine:        -10 -> -0.40\n",
    "    - Total flags > mines:  -10 -> -0.40\n",
    "    - Reveal mine:          -25 -> -1.00\n",
    "    - Flag correct mine:    +15 -> +0.60\n",
    "    - Reveal safe (random): +10 -> +0.40\n",
    "    - Reveal safe (deducible): +15 -> +0.60\n",
    "    - Win game:            +37.5 -> +1.50 (capped)\n",
    "    \"\"\"\n",
    "    mine_positions_list = kwargs.get(\"mine_positions\", [])\n",
    "    rows_list = kwargs.get(\"rows\", [])\n",
    "    cols_list = kwargs.get(\"cols\", [])\n",
    "    num_mines_list = kwargs.get(\"num_mines\", [])\n",
    "    flagged_positions_list = kwargs.get(\"flagged_positions\", [])\n",
    "    _revealed_positions_list = kwargs.get(\"revealed_positions\", [])\n",
    "    deducible_moves_list = kwargs.get(\"deducible_moves\", [])\n",
    "\n",
    "    scores = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = (\n",
    "            completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        )\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(-10.0 / 25.0)  # Invalid format penalty\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Get stored game data\n",
    "            mine_pos = (\n",
    "                json.loads(mine_positions_list[idx])\n",
    "                if isinstance(mine_positions_list[idx], str)\n",
    "                else mine_positions_list[idx]\n",
    "            )\n",
    "            rows = int(rows_list[idx])\n",
    "            cols = int(cols_list[idx])\n",
    "            num_mines = int(num_mines_list[idx])\n",
    "            flagged_pos = (\n",
    "                json.loads(flagged_positions_list[idx])\n",
    "                if isinstance(flagged_positions_list[idx], str)\n",
    "                else flagged_positions_list[idx]\n",
    "            )\n",
    "            revealed_pos = (\n",
    "                json.loads(_revealed_positions_list[idx])\n",
    "                if isinstance(_revealed_positions_list[idx], str)\n",
    "                else _revealed_positions_list[idx]\n",
    "            )\n",
    "            deducible_raw = (\n",
    "                json.loads(deducible_moves_list[idx])\n",
    "                if isinstance(deducible_moves_list[idx], str)\n",
    "                else deducible_moves_list[idx]\n",
    "            )\n",
    "\n",
    "            mine_set = set(tuple(p) for p in mine_pos)\n",
    "            flagged_set = set(tuple(p) for p in flagged_pos)\n",
    "            revealed_set = set(tuple(p) for p in revealed_pos)\n",
    "            deducible_set = set((m[0], m[1], m[2]) for m in deducible_raw)\n",
    "\n",
    "            row, col = action[\"row\"], action[\"col\"]\n",
    "            action_type = action[\"type\"]\n",
    "\n",
    "            # Out of bounds\n",
    "            if not (0 <= row < rows and 0 <= col < cols):\n",
    "                scores.append(-15.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            # Already revealed\n",
    "            if (row, col) in revealed_set:\n",
    "                scores.append(-12.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            # Already flagged\n",
    "            if (row, col) in flagged_set:\n",
    "                scores.append(-8.0 / 25.0)\n",
    "                continue\n",
    "\n",
    "            if action_type == \"flag\":\n",
    "                # Total flags > total mines check\n",
    "                if len(flagged_set) >= num_mines:\n",
    "                    scores.append(-10.0 / 25.0)\n",
    "                    continue\n",
    "\n",
    "                if (row, col) in mine_set:\n",
    "                    # Flag correct mine - simulate to check win\n",
    "                    game = reconstruct_game(\n",
    "                        mine_pos, rows, cols, revealed_pos, flagged_pos\n",
    "                    )\n",
    "                    game.flag(row, col)\n",
    "                    # Win: all mines flagged AND all safe revealed\n",
    "                    safe_total = rows * cols - num_mines\n",
    "                    if (\n",
    "                        len(game.flagged) == num_mines\n",
    "                        and len(game.revealed) >= safe_total\n",
    "                    ):\n",
    "                        scores.append(37.5 / 25.0)  # Capped win reward\n",
    "                    else:\n",
    "                        scores.append(15.0 / 25.0)\n",
    "                else:\n",
    "                    scores.append(-10.0 / 25.0)  # Flag non-mine\n",
    "\n",
    "            elif action_type == \"reveal\":\n",
    "                if (row, col) in mine_set:\n",
    "                    scores.append(-25.0 / 25.0)  # Hit mine\n",
    "                else:\n",
    "                    is_deducible = (\"reveal\", row, col) in deducible_set\n",
    "\n",
    "                    # Simulate the reveal with flood fill to detect cascade wins\n",
    "                    game = reconstruct_game(\n",
    "                        mine_pos, rows, cols, revealed_pos, flagged_pos\n",
    "                    )\n",
    "                    game.reveal(row, col)\n",
    "\n",
    "                    # Win: all safe revealed (checked by game.reveal) AND all mines flagged\n",
    "                    safe_total = rows * cols - num_mines\n",
    "                    if (\n",
    "                        len(game.revealed) >= safe_total\n",
    "                        and len(game.flagged) == num_mines\n",
    "                    ):\n",
    "                        scores.append(37.5 / 25.0)  # Capped win reward\n",
    "                    elif is_deducible:\n",
    "                        scores.append(15.0 / 25.0)\n",
    "                    else:\n",
    "                        scores.append(10.0 / 25.0)\n",
    "\n",
    "        except Exception:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Reward Function 3: Strategic Reward (weight: 0.5)\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "def strategic_reward(completions, **kwargs):\n",
    "    \"\"\"Reward strategic play quality.\n",
    "\n",
    "    - Guessed when deducible move existed: -0.3\n",
    "    - Move adjacent to revealed numbers:  +0.2\n",
    "    - Flagged certain mine with safe reveals available: +0.15\n",
    "    - Over-flagged (flags >= mines and chose flag): -0.4\n",
    "    - Reveal triggers 0-cell cascade: +0.15\n",
    "    \"\"\"\n",
    "    deducible_moves_list = kwargs.get(\"deducible_moves\", [])\n",
    "    _is_deducible_list = kwargs.get(\"is_deducible\", [])\n",
    "    mine_positions_list = kwargs.get(\"mine_positions\", [])\n",
    "    rows_list = kwargs.get(\"rows\", [])\n",
    "    cols_list = kwargs.get(\"cols\", [])\n",
    "    num_mines_list = kwargs.get(\"num_mines\", [])\n",
    "    flagged_positions_list = kwargs.get(\"flagged_positions\", [])\n",
    "    _revealed_positions_list = kwargs.get(\"revealed_positions\", [])\n",
    "    board_state_list = kwargs.get(\"board_state\", [])\n",
    "\n",
    "    scores = []\n",
    "    for idx, completion in enumerate(completions):\n",
    "        response = (\n",
    "            completion[0][\"content\"] if isinstance(completion, list) else completion\n",
    "        )\n",
    "        action = parse_llm_action(response)\n",
    "\n",
    "        if action is None:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rows = int(rows_list[idx])\n",
    "            cols = int(cols_list[idx])\n",
    "            num_mines = int(num_mines_list[idx])\n",
    "            deducible_raw = (\n",
    "                json.loads(deducible_moves_list[idx])\n",
    "                if isinstance(deducible_moves_list[idx], str)\n",
    "                else deducible_moves_list[idx]\n",
    "            )\n",
    "            flagged_pos = (\n",
    "                json.loads(flagged_positions_list[idx])\n",
    "                if isinstance(flagged_positions_list[idx], str)\n",
    "                else flagged_positions_list[idx]\n",
    "            )\n",
    "            board = (\n",
    "                json.loads(board_state_list[idx])\n",
    "                if isinstance(board_state_list[idx], str)\n",
    "                else board_state_list[idx]\n",
    "            )\n",
    "            mine_pos = (\n",
    "                json.loads(mine_positions_list[idx])\n",
    "                if isinstance(mine_positions_list[idx], str)\n",
    "                else mine_positions_list[idx]\n",
    "            )\n",
    "            mine_set = set(tuple(p) for p in mine_pos)\n",
    "            flagged_set = set(tuple(p) for p in flagged_pos)\n",
    "\n",
    "            row, col = action[\"row\"], action[\"col\"]\n",
    "            action_type = action[\"type\"]\n",
    "            score = 0.0\n",
    "\n",
    "            # Check bounds\n",
    "            if not (0 <= row < rows and 0 <= col < cols):\n",
    "                scores.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Check if there were deducible moves\n",
    "            has_deducible = len(deducible_raw) > 0\n",
    "            action_is_deducible = False\n",
    "            for m in deducible_raw:\n",
    "                if m[0] == action_type and m[1] == row and m[2] == col:\n",
    "                    action_is_deducible = True\n",
    "                    break\n",
    "\n",
    "            # Penalty: guessed when deducible move existed\n",
    "            if has_deducible and not action_is_deducible:\n",
    "                score -= 0.3\n",
    "\n",
    "            # Reward: move adjacent to revealed numbers (info-gathering)\n",
    "            adjacent_to_number = False\n",
    "            for dr in [-1, 0, 1]:\n",
    "                for dc in [-1, 0, 1]:\n",
    "                    nr, nc = row + dr, col + dc\n",
    "                    if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                        if board[nr][nc] in \"012345678\":\n",
    "                            adjacent_to_number = True\n",
    "                            break\n",
    "                if adjacent_to_number:\n",
    "                    break\n",
    "            if adjacent_to_number:\n",
    "                score += 0.2\n",
    "\n",
    "            # Reward: flagged certain mine (flag-first strategy)\n",
    "            if action_type == \"flag\" and (row, col) in mine_set and action_is_deducible:\n",
    "                score += 0.15\n",
    "\n",
    "            # Penalty: over-flagging\n",
    "            if action_type == \"flag\" and len(flagged_set) >= num_mines:\n",
    "                score -= 0.4\n",
    "\n",
    "            # Reward: reveal that triggers 0-cell cascade\n",
    "            if action_type == \"reveal\" and (row, col) not in mine_set:\n",
    "                adj_mine_count = 0\n",
    "                for dr in [-1, 0, 1]:\n",
    "                    for dc in [-1, 0, 1]:\n",
    "                        if dr == 0 and dc == 0:\n",
    "                            continue\n",
    "                        nr, nc = row + dr, col + dc\n",
    "                        if 0 <= nr < rows and 0 <= nc < cols and (nr, nc) in mine_set:\n",
    "                            adj_mine_count += 1\n",
    "                if adj_mine_count == 0:\n",
    "                    score += 0.15  # Cell is 0 -> triggers flood-fill cascade\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        except Exception:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "print(\"Reward functions defined:\")\n",
    "print(\"  1. format_reward (weight=1.0): JSON validity\")\n",
    "print(\n",
    "    \"  2. gameplay_reward (weight=2.0): Game rules + flood-fill win detection (requires all flags)\"\n",
    ")\n",
    "print(\"  3. strategic_reward (weight=0.5): Strategic play + cascade detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Prepare GRPO Dataset & Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare GRPO dataset with all metadata columns\n",
    "# IMPORTANT: \"prompt\" must be a list of dicts (not JSON string) for TRL\n",
    "grpo_items = []\n",
    "skipped_long = 0\n",
    "MAX_PROMPT_TOKENS = (\n",
    "    7500  # Leave headroom: 8192 - 128 (completion) - 564 (padding/overhead)\n",
    ")\n",
    "\n",
    "for ex in raw_data:\n",
    "    prompt_msgs = json.loads(ex[\"prompt\"])  # Parse JSON string -> list of dicts\n",
    "\n",
    "    # Filter out prompts that would be truncated by TRL's max_prompt_length\n",
    "    # This prevents silent reward poisoning where model sees truncated board\n",
    "    # but reward function scores against full game state\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        prompt_msgs, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    token_len = len(tokenizer(text, add_special_tokens=False).input_ids)\n",
    "    if token_len > MAX_PROMPT_TOKENS:\n",
    "        skipped_long += 1\n",
    "        continue\n",
    "\n",
    "    item = {\n",
    "        \"prompt\": prompt_msgs,\n",
    "        \"mine_positions\": ex[\"mine_positions\"],\n",
    "        \"rows\": ex[\"rows\"],\n",
    "        \"cols\": ex[\"cols\"],\n",
    "        \"num_mines\": ex[\"num_mines\"],\n",
    "        \"flagged_positions\": ex[\"flagged_positions\"],\n",
    "        \"revealed_positions\": ex[\"revealed_positions\"],\n",
    "        \"board_state\": ex[\"board_state\"],\n",
    "        \"deducible_moves\": ex[\"deducible_moves\"],\n",
    "        \"best_move\": ex[\"best_move\"],\n",
    "        \"is_deducible\": ex[\"is_deducible\"],\n",
    "    }\n",
    "    grpo_items.append(item)\n",
    "\n",
    "grpo_dataset = Dataset.from_list(grpo_items)\n",
    "print(\n",
    "    f\"GRPO dataset: {len(grpo_dataset)} examples (filtered {skipped_long} long prompts > {MAX_PROMPT_TOKENS} tokens)\"\n",
    ")\n",
    "print(f\"Columns: {grpo_dataset.column_names}\")\n",
    "\n",
    "# Verify prompt format\n",
    "assert isinstance(grpo_dataset[0][\"prompt\"], list), \"Prompt must be a list!\"\n",
    "assert isinstance(grpo_dataset[0][\"prompt\"][0], dict), \"Each prompt msg must be a dict!\"\n",
    "print(\"Prompt format check passed\")\n",
    "\n",
    "# Show token length distribution of remaining data\n",
    "if len(grpo_items) > 0:\n",
    "    sample_lens = []\n",
    "    for item in grpo_items[:1000]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            item[\"prompt\"], tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        sample_lens.append(len(tokenizer(text, add_special_tokens=False).input_ids))\n",
    "    print(\n",
    "        f\"Token length stats (sample of {len(sample_lens)}): min={min(sample_lens)} max={max(sample_lens)} mean={sum(sample_lens) / len(sample_lens):.0f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch model back to training mode\n",
    "FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class CheckpointCallback(TrainerCallback):\n",
    "    \"\"\"Save ablation checkpoints at specific steps.\"\"\"\n",
    "\n",
    "    def __init__(self, save_steps_list):\n",
    "        self.save_steps_list = set(save_steps_list)\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        if state.global_step in self.save_steps_list and model is not None:\n",
    "            ckpt_dir = f\"grpo_{state.global_step}\"\n",
    "            model.save_pretrained(ckpt_dir)\n",
    "            print(f\"\\nSaved ablation checkpoint: {ckpt_dir}\")\n",
    "\n",
    "\n",
    "checkpoint_cb = CheckpointCallback(save_steps_list=[400, 800])\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"grpo_outputs\",\n",
    "    # DAPO loss with asymmetric clipping\n",
    "    loss_type=\"dapo\",\n",
    "    epsilon=0.2,\n",
    "    epsilon_high=0.28,\n",
    "    beta=0.0,  # No KL, no ref model\n",
    "    # Generation\n",
    "    num_generations=8,\n",
    "    max_prompt_length=7500,  # Matches token filter in cell 18 - no silent truncation\n",
    "    max_completion_length=128,\n",
    "    temperature=1.0,\n",
    "    # Training\n",
    "    per_device_train_batch_size=1,  # Reduced for 8K prompts + 8 generations\n",
    "    gradient_accumulation_steps=8,  # Effective batch = 8\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    max_steps=1200,\n",
    "    # Optimization\n",
    "    optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    # Reward\n",
    "    scale_rewards=\"batch\",\n",
    "    reward_weights=[1.0, 2.0, 0.5],\n",
    "    # Generation masking\n",
    "    mask_truncated_completions=True,\n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    save_steps=400,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    "    # Disable vLLM - Unsloth LoRA model doesn't expose vllm_engine attribute\n",
    "    use_vllm=False,\n",
    ")\n",
    "\n",
    "print(\"GRPO config:\")\n",
    "print(\n",
    "    f\"  Loss: {grpo_config.loss_type}, epsilon={grpo_config.epsilon}/{grpo_config.epsilon_high}\"\n",
    ")\n",
    "print(f\"  Generations: {grpo_config.num_generations}\")\n",
    "print(\n",
    "    f\"  Batch: {grpo_config.per_device_train_batch_size} x {grpo_config.gradient_accumulation_steps}\"\n",
    ")\n",
    "print(f\"  Steps: {grpo_config.max_steps}\")\n",
    "print(f\"  LR: {grpo_config.learning_rate}\")\n",
    "print(f\"  Reward weights: {grpo_config.reward_weights}\")\n",
    "print(f\"  vLLM: {grpo_config.use_vllm}\")\n",
    "print(f\"  Max prompt length: {grpo_config.max_prompt_length}\")\n",
    "print(f\"  Max completion length: {grpo_config.max_completion_length}\")\n",
    "\n",
    "# Create trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[format_reward, gameplay_reward, strategic_reward],\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_dataset,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GRPO training...\")\n",
    "try:\n",
    "    grpo_trainer.train()\n",
    "    print(\"GRPO training complete!\")\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\nOOM with 8 generations! Falling back to num_generations=4...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Rebuild with reduced generations\n",
    "    grpo_config_fallback = GRPOConfig(\n",
    "        output_dir=\"grpo_outputs\",\n",
    "        loss_type=\"dapo\",\n",
    "        epsilon=0.2,\n",
    "        epsilon_high=0.28,\n",
    "        beta=0.0,\n",
    "        num_generations=4,  # Reduced from 8\n",
    "        max_prompt_length=7500,\n",
    "        max_completion_length=128,\n",
    "        temperature=1.0,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=5e-6,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=1200,\n",
    "        optim=\"adamw_8bit\",\n",
    "        bf16=True,\n",
    "        scale_rewards=\"batch\",\n",
    "        reward_weights=[1.0, 2.0, 0.5],\n",
    "        mask_truncated_completions=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=400,\n",
    "        save_total_limit=3,\n",
    "        report_to=\"none\",\n",
    "        use_vllm=False,\n",
    "    )\n",
    "    FastLanguageModel.for_training(model)\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=[format_reward, gameplay_reward, strategic_reward],\n",
    "        args=grpo_config_fallback,\n",
    "        train_dataset=grpo_dataset,\n",
    "        callbacks=[checkpoint_cb],\n",
    "    )\n",
    "    grpo_trainer.train()\n",
    "    print(\"GRPO training complete (with fallback config)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate GRPO model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"Post-GRPO evaluation:\")\n",
    "grpo_results = quick_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    board_configs=[\n",
    "        (6, 6, 5, 10),\n",
    "        (10, 10, 15, 10),\n",
    "        (16, 16, 40, 5),\n",
    "        (20, 20, 60, 5),\n",
    "        (30, 30, 120, 3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compare with SFT results\n",
    "print(\"\\nComparison (JSON% / ValidMove% / Wins):\")\n",
    "for size in sorted(\n",
    "    set(list(sft_results.keys()) + list(grpo_results.keys())),\n",
    "    key=lambda x: int(x.split(\"x\")[0]),\n",
    "):\n",
    "    sft_r = sft_results.get(size, (0, 0, 0, 0))\n",
    "    grpo_r = grpo_results.get(size, (0, 0, 0, 0))\n",
    "    print(\n",
    "        f\"  {size}: SFT={sft_r[0]:.0f}%/{sft_r[1]:.0f}%/W{sft_r[3]}  GRPO={grpo_r[0]:.0f}%/{grpo_r[1]:.0f}%/W{grpo_r[3]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 12: Save Final Merged Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged model in 16-bit for evaluation\n",
    "output_path = \"/workspace/your_finetuned_model\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    output_path,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"Model saved to: {output_path}\")\n",
    "print(\"This path is referenced in agents/minesweeper_model.py\")\n",
    "\n",
    "# Verify the saved model\n",
    "import os\n",
    "\n",
    "model_files = os.listdir(output_path)\n",
    "print(f\"\\nFiles: {model_files}\")\n",
    "total_size = sum(\n",
    "    os.path.getsize(os.path.join(output_path, f))\n",
    "    for f in model_files\n",
    "    if os.path.isfile(os.path.join(output_path, f))\n",
    ")\n",
    "print(f\"Total size: {total_size / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive final evaluation with full game playouts\n",
    "# Use higher move limit for meaningful win rate data (unlike quick_eval's 10-move sanity check)\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def full_eval(model, tokenizer, board_configs, max_moves_per_game=500):\n",
    "    \"\"\"Full evaluation with high move limit for actual win rate measurement.\n",
    "\n",
    "    Includes:\n",
    "    - Infinite loop detection (break after 3 identical consecutive actions)\n",
    "    - Accurate mine_hit tracking (not counted as valid_moves)\n",
    "    - Continues after invalid moves (like competition)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for rows, cols, mines, n_games in board_configs:\n",
    "        valid_json = 0\n",
    "        valid_moves = 0\n",
    "        invalid_moves = 0\n",
    "        total_moves = 0\n",
    "        wins = 0\n",
    "        mine_hits = 0\n",
    "        loops_broken = 0\n",
    "\n",
    "        for seed in range(n_games):\n",
    "            rng = random.Random(seed + 20000)\n",
    "            positions = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "            mine_pos = rng.sample(positions, mines)\n",
    "            game = MinesweeperGame(rows, cols, mine_pos)\n",
    "\n",
    "            safe = [\n",
    "                (r, c)\n",
    "                for r in range(rows)\n",
    "                for c in range(cols)\n",
    "                if (r, c) not in game.mine_set\n",
    "            ]\n",
    "            first = rng.choice(safe)\n",
    "            game.reveal(*first)\n",
    "\n",
    "            last_action = None\n",
    "            repeat_count = 0\n",
    "\n",
    "            for move_i in range(max_moves_per_game):\n",
    "                if game.state != \"ongoing\":\n",
    "                    if game.state == \"success\":\n",
    "                        wins += 1\n",
    "                    break\n",
    "\n",
    "                board = game.get_board()\n",
    "                flags = len(game.flagged)\n",
    "                prompt = build_eval_prompt(board, rows, cols, mines, flags)\n",
    "\n",
    "                sys_prompt = \"You are an expert Minesweeper AI. Analyze constraints and output ONLY a valid JSON action. No explanation.\"\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "                text = tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "                with torch.no_grad():\n",
    "                    output = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=64,\n",
    "                        temperature=1.0,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                    )\n",
    "                response = tokenizer.decode(\n",
    "                    output[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "                )\n",
    "                action = parse_llm_action(response)\n",
    "\n",
    "                # Infinite loop detection: if same action 3 times in a row, break\n",
    "                if action is not None:\n",
    "                    action_tuple = (action[\"type\"], action[\"row\"], action[\"col\"])\n",
    "                    if action_tuple == last_action:\n",
    "                        repeat_count += 1\n",
    "                        if repeat_count >= 3:\n",
    "                            loops_broken += 1\n",
    "                            break\n",
    "                    else:\n",
    "                        repeat_count = 1\n",
    "                        last_action = action_tuple\n",
    "\n",
    "                total_moves += 1\n",
    "                if action is not None:\n",
    "                    valid_json += 1\n",
    "                    r_act, c_act = action[\"row\"], action[\"col\"]\n",
    "                    if 0 <= r_act < rows and 0 <= c_act < cols:\n",
    "                        cell_val = board[r_act][c_act]\n",
    "                        if cell_val == \".\":\n",
    "                            if action[\"type\"] == \"reveal\":\n",
    "                                result = game.reveal(r_act, c_act)\n",
    "                                if result == \"mine\":\n",
    "                                    mine_hits += 1\n",
    "                                    # Mine hit is NOT a \"valid move\" for metrics\n",
    "                                else:\n",
    "                                    valid_moves += 1\n",
    "                            elif action[\"type\"] == \"flag\":\n",
    "                                game.flag(r_act, c_act)\n",
    "                                valid_moves += 1\n",
    "                        else:\n",
    "                            invalid_moves += 1\n",
    "                    else:\n",
    "                        invalid_moves += 1\n",
    "                else:\n",
    "                    invalid_moves += 1\n",
    "\n",
    "        json_rate = valid_json / max(total_moves, 1) * 100\n",
    "        move_rate = valid_moves / max(total_moves, 1) * 100\n",
    "        results[f\"{rows}x{cols}\"] = (\n",
    "            json_rate,\n",
    "            move_rate,\n",
    "            total_moves,\n",
    "            wins,\n",
    "            mine_hits,\n",
    "            n_games,\n",
    "        )\n",
    "        loop_str = f\" Loops={loops_broken}\" if loops_broken > 0 else \"\"\n",
    "        print(\n",
    "            f\"  {rows}x{cols}: JSON={json_rate:.0f}% ValidMove={move_rate:.0f}% Wins={wins}/{n_games} MineHits={mine_hits} Invalid={invalid_moves}{loop_str} ({total_moves} moves)\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "final_results = full_eval(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    board_configs=[\n",
    "        (6, 6, 5, 20),\n",
    "        (8, 8, 10, 20),\n",
    "        (10, 10, 15, 20),\n",
    "        (16, 16, 40, 10),\n",
    "        (20, 20, 60, 10),\n",
    "        (30, 30, 120, 5),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for size, (json_r, move_r, total, wins, mine_hits, n_games) in sorted(\n",
    "    final_results.items(), key=lambda x: int(x[0].split(\"x\")[0])\n",
    "):\n",
    "    status = \"PASS\" if json_r >= 90 and move_r >= 50 else \"CHECK\"\n",
    "    print(\n",
    "        f\"  [{status}] {size}: JSON={json_r:.0f}% ValidMove={move_r:.0f}% Wins={wins}/{n_games} MineHits={mine_hits} ({total} moves)\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining pipeline complete! Model saved to /workspace/your_finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done! Summary of what was trained:\n",
    "#\n",
    "# Model: Qwen2.5-14B-Instruct with LoRA (rank=64, alpha=128)\n",
    "# Phase 1: SFT on 50K solver-generated examples (1 epoch)\n",
    "# Phase 2: GRPO with 3 reward functions (1200 steps, DAPO loss)\n",
    "#\n",
    "# Reward functions:\n",
    "#   1. format_reward: Valid JSON output (+1.0 / -3.0)\n",
    "#   2. gameplay_reward: Game rules scoring (normalized /25)\n",
    "#   3. strategic_reward: Strategic play quality (deducible moves, flag-first)\n",
    "#\n",
    "# Output: /workspace/your_finetuned_model (merged 16-bit)\n",
    "# Agent: /workspace/agents/ (minesweeper_model.py points to model)\n",
    "#\n",
    "# Key decisions:\n",
    "# - Compact grid prompt for <=16x16, frontier sparse for >16x16\n",
    "# - Greedy decoding at eval (temperature=0, do_sample=false)\n",
    "# - Flag-first strategy (flag certain mines before revealing safe cells)\n",
    "# - Win reward capped at +1.5 normalized to prevent gradient spikes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}